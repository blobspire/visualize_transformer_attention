# Visualize Transformer Attention


## Overview
This project uses a `BERT` model from `HuggingFace` to generate self-attention weights for given text input. It then visualizes these weights through a heatmap and a connected graph to demonstrate how the self-attention mechanism in transformer models identifies the semantic relationships between tokens.


## Requirements
* transformers
* matplotlib
* numpy
* networkx